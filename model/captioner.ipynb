{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Decoder_RNN import DecoderRNN\n",
    "from Encoder_CNN import EncoderCNN\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import kagglehub\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import spacy\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download latest version\n",
    "#path = kagglehub.dataset_download(\"adityajn105/flickr8k\")\n",
    "\n",
    "#print(\"Path to dataset files:\", path)\n",
    "#/home/esun/.cache/kagglehub/datasets/adityajn105/flickr8k/versions/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNtoRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        super(CNNtoRNN, self).__init__()\n",
    "        self.encoderCNN = EncoderCNN(embed_size)\n",
    "        self.decoderRNN = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoderCNN(images)\n",
    "        outputs = self.decoderRNN(features, captions)\n",
    "        return outputs\n",
    "\n",
    "    def caption_image(self, image, vocabulary, max_length=50):\n",
    "        result_caption = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = self.encoderCNN(image).unsqueeze(0)\n",
    "            states = None\n",
    "\n",
    "            for _ in range(max_length):\n",
    "                hiddens, states = self.decoderRNN.lstm(x, states)\n",
    "                output = self.decoderRNN.linear(hiddens.squeeze(0))\n",
    "                predicted = output.argmax(1)\n",
    "                result_caption.append(predicted.item())\n",
    "                x = self.decoderRNN.embed(predicted).unsqueeze(0)\n",
    "\n",
    "                if vocabulary.itos[predicted.item()] == \"<EOS>\":\n",
    "                    break\n",
    "\n",
    "        return [vocabulary.itos[idx] for idx in result_caption]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold):\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "        self.freq_threshold = freq_threshold\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenizer_eng(text):\n",
    "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
    "\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        frequencies = {}\n",
    "        idx = 4\n",
    "\n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenizer_eng(sentence):\n",
    "                if word not in frequencies:\n",
    "                    frequencies[word] = 1\n",
    "\n",
    "                else:\n",
    "                    frequencies[word] += 1\n",
    "\n",
    "                if frequencies[word] == self.freq_threshold:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenizer_eng(text)\n",
    "\n",
    "        return [\n",
    "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
    "            for token in tokenized_text\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, root_dir, captions_file, transform=None, freq_threshold=5):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = pd.read_csv(captions_file)\n",
    "        self.transform = transform\n",
    "\n",
    "        # Get img, caption columns\n",
    "        self.imgs = self.df[\"image\"]\n",
    "        self.captions = self.df[\"caption\"]\n",
    "\n",
    "        # Initialize vocabulary and build vocab\n",
    "        self.vocab = Vocabulary(freq_threshold)\n",
    "        self.vocab.build_vocabulary(self.captions.tolist())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        caption = self.captions[index]\n",
    "        img_id = self.imgs[index]\n",
    "        img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_caption += self.vocab.numericalize(caption)\n",
    "        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n",
    "\n",
    "        return img, torch.tensor(numericalized_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_eng = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCollate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
    "        imgs = torch.cat(imgs, dim=0)\n",
    "        targets = [item[1] for item in batch]\n",
    "        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_idx)\n",
    "\n",
    "        return imgs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "root=\"Flickr8k/Images/\"\n",
    "captions=\"Flickr8k/captions.txt\"\n",
    "transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomResizedCrop((224, 224)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ]\n",
    "    )\n",
    "dataset=FlickrDataset(root,captions,transform)\n",
    "pad_idx=dataset.vocab.stoi[\"<PAD>\"]\n",
    "loader=DataLoader(dataset,batch_size=32,num_workers=2,shuffle=True,pin_memory=True,collate_fn=MyCollate(pad_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.9608, -0.9608, -0.9608,  ..., -0.7882, -0.7725, -0.7725],\n",
      "          [-0.9608, -0.9608, -0.9608,  ..., -0.7647, -0.7569, -0.7176],\n",
      "          [-0.9608, -0.9608, -0.9608,  ..., -0.7255, -0.7647, -0.7255],\n",
      "          ...,\n",
      "          [-0.1765, -0.1608, -0.1843,  ...,  0.0745,  0.0824,  0.1294],\n",
      "          [-0.1686, -0.1922, -0.2000,  ...,  0.0745,  0.0667,  0.1137],\n",
      "          [-0.1765, -0.2235, -0.2000,  ...,  0.1059,  0.1137,  0.1059]],\n",
      "\n",
      "         [[-0.9608, -0.9608, -0.9608,  ..., -0.8824, -0.8824, -0.8667],\n",
      "          [-0.9608, -0.9608, -0.9608,  ..., -0.8510, -0.8745, -0.8588],\n",
      "          [-0.9608, -0.9608, -0.9608,  ..., -0.7961, -0.8667, -0.8588],\n",
      "          ...,\n",
      "          [-0.0902, -0.0745, -0.0902,  ...,  0.0902,  0.0980,  0.1137],\n",
      "          [-0.0824, -0.0902, -0.0824,  ...,  0.0902,  0.0667,  0.0980],\n",
      "          [-0.0824, -0.0980, -0.0902,  ...,  0.1216,  0.0980,  0.1059]],\n",
      "\n",
      "         [[-0.9608, -0.9608, -0.9608,  ..., -0.8902, -0.8824, -0.9059],\n",
      "          [-0.9608, -0.9608, -0.9608,  ..., -0.8745, -0.8745, -0.8824],\n",
      "          [-0.9608, -0.9608, -0.9608,  ..., -0.8353, -0.8902, -0.8902],\n",
      "          ...,\n",
      "          [-0.0745, -0.0667, -0.0745,  ...,  0.0667,  0.0588,  0.0745],\n",
      "          [-0.0667, -0.0824, -0.0667,  ...,  0.0588,  0.0431,  0.0588],\n",
      "          [-0.0745, -0.0902, -0.0667,  ...,  0.0824,  0.0824,  0.0667]]],\n",
      "\n",
      "\n",
      "        [[[-0.6157, -0.6784, -0.8039,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [-0.3569, -0.4588, -0.6471,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 0.0980, -0.0588, -0.3647,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          ...,\n",
      "          [-0.9686, -0.9765, -0.9843,  ..., -0.9765, -0.9686, -0.9686],\n",
      "          [-0.9686, -0.9765, -0.9843,  ..., -0.9686, -0.9608, -0.9608],\n",
      "          [-0.9686, -0.9765, -0.9843,  ..., -0.9608, -0.9608, -0.9608]],\n",
      "\n",
      "         [[-0.8745, -0.8745, -0.8824,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [-0.8510, -0.8588, -0.8745,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [-0.8118, -0.8275, -0.8667,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          ...,\n",
      "          [-0.9686, -0.9686, -0.9765,  ..., -0.9216, -0.9137, -0.9137],\n",
      "          [-0.9686, -0.9686, -0.9686,  ..., -0.8980, -0.8902, -0.8902],\n",
      "          [-0.9686, -0.9686, -0.9686,  ..., -0.8824, -0.8824, -0.8824]],\n",
      "\n",
      "         [[-0.8588, -0.8667, -0.8824,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [-0.7961, -0.8118, -0.8510,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [-0.6863, -0.7255, -0.8039,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          ...,\n",
      "          [-0.9686, -0.9765, -0.9843,  ..., -0.9373, -0.9294, -0.9294],\n",
      "          [-0.9686, -0.9686, -0.9765,  ..., -0.9137, -0.9216, -0.9216],\n",
      "          [-0.9686, -0.9686, -0.9765,  ..., -0.9059, -0.9137, -0.9137]]],\n",
      "\n",
      "\n",
      "        [[[-0.0431, -0.0275, -0.0588,  ...,  0.1608,  0.1765,  0.1765],\n",
      "          [-0.0353, -0.0510, -0.0353,  ...,  0.1451,  0.1843,  0.1922],\n",
      "          [-0.0196, -0.0275, -0.0118,  ...,  0.1373,  0.1529,  0.1529],\n",
      "          ...,\n",
      "          [-0.1922, -0.1686, -0.0039,  ...,  0.2078,  0.1294,  0.1294],\n",
      "          [-0.1451, -0.1294,  0.0275,  ...,  0.1922,  0.1529,  0.1137],\n",
      "          [-0.1451, -0.1059,  0.0510,  ...,  0.2157,  0.1608,  0.0902]],\n",
      "\n",
      "         [[ 0.2157,  0.2157,  0.1686,  ...,  0.3647,  0.3725,  0.3412],\n",
      "          [ 0.2157,  0.1922,  0.2000,  ...,  0.3725,  0.3961,  0.3647],\n",
      "          [ 0.2392,  0.2157,  0.2157,  ...,  0.3647,  0.3647,  0.3412],\n",
      "          ...,\n",
      "          [ 0.0510,  0.0745,  0.2235,  ...,  0.3961,  0.3333,  0.3804],\n",
      "          [ 0.0902,  0.0902,  0.2314,  ...,  0.3961,  0.3647,  0.3647],\n",
      "          [ 0.0667,  0.0980,  0.2471,  ...,  0.4275,  0.3882,  0.3412]],\n",
      "\n",
      "         [[-0.6784, -0.6157, -0.6235,  ..., -0.4353, -0.4196, -0.3882],\n",
      "          [-0.6784, -0.6392, -0.5922,  ..., -0.4745, -0.4275, -0.4039],\n",
      "          [-0.6627, -0.6157, -0.5529,  ..., -0.4902, -0.4353, -0.4510],\n",
      "          ...,\n",
      "          [-0.8353, -0.8118, -0.6392,  ..., -0.4745, -0.5451, -0.4588],\n",
      "          [-0.7725, -0.7490, -0.6157,  ..., -0.4745, -0.4980, -0.4588],\n",
      "          [-0.7569, -0.7098, -0.5686,  ..., -0.4353, -0.4745, -0.4824]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.3490, -0.3569, -0.3725,  ..., -0.5608, -0.8588, -0.8902],\n",
      "          [-0.3647, -0.3725, -0.3725,  ..., -0.7647, -0.8902, -0.8275],\n",
      "          [-0.3804, -0.3804, -0.3725,  ..., -0.8431, -0.8824, -0.7961],\n",
      "          ...,\n",
      "          [-0.9373, -0.9137, -0.9059,  ..., -0.9922, -0.9843, -0.9922],\n",
      "          [-0.7882, -0.7333, -0.7255,  ..., -1.0000, -0.9843, -0.9843],\n",
      "          [-0.8118, -0.7569, -0.7882,  ..., -0.9843, -0.9451, -0.9608]],\n",
      "\n",
      "         [[-0.3020, -0.2863, -0.2549,  ..., -0.7333, -0.9216, -0.9373],\n",
      "          [-0.3020, -0.2706, -0.2392,  ..., -0.8745, -0.9373, -0.8902],\n",
      "          [-0.3020, -0.2627, -0.2157,  ..., -0.9137, -0.9216, -0.8510],\n",
      "          ...,\n",
      "          [-0.3412, -0.3333, -0.3333,  ...,  0.0353,  0.0275,  0.0353],\n",
      "          [-0.2314, -0.2000, -0.1765,  ...,  0.0118,  0.0196,  0.0196],\n",
      "          [-0.2078, -0.1765, -0.1843,  ...,  0.0353,  0.0510,  0.0431]],\n",
      "\n",
      "         [[-0.2706, -0.2235, -0.1451,  ..., -0.8824, -0.9686, -0.9608],\n",
      "          [-0.2392, -0.1765, -0.0902,  ..., -0.9373, -0.9608, -0.9373],\n",
      "          [-0.2235, -0.1451, -0.0510,  ..., -0.9451, -0.9451, -0.9137],\n",
      "          ...,\n",
      "          [-0.1137, -0.1137, -0.1137,  ...,  0.3333,  0.3333,  0.3255],\n",
      "          [-0.0745, -0.0588, -0.0431,  ...,  0.3176,  0.3255,  0.3333],\n",
      "          [-0.0902, -0.0510, -0.0588,  ...,  0.3569,  0.3725,  0.3647]]],\n",
      "\n",
      "\n",
      "        [[[-0.2078, -0.2941, -0.3804,  ..., -0.2314, -0.3804, -0.5373],\n",
      "          [-0.2784, -0.3490, -0.4039,  ..., -0.2392, -0.3098, -0.3804],\n",
      "          [-0.3490, -0.4118, -0.4667,  ..., -0.2235, -0.2784, -0.3020],\n",
      "          ...,\n",
      "          [-0.2157, -0.2863, -0.3882,  ..., -0.6471, -0.6549, -0.6549],\n",
      "          [-0.3098, -0.3255, -0.3725,  ..., -0.6471, -0.6549, -0.6392],\n",
      "          [-0.3882, -0.3725, -0.3804,  ..., -0.6314, -0.6314, -0.6157]],\n",
      "\n",
      "         [[ 0.0118, -0.0980, -0.2235,  ..., -0.1059, -0.2784, -0.4667],\n",
      "          [-0.0353, -0.1294, -0.2157,  ..., -0.0902, -0.1529, -0.2078],\n",
      "          [-0.1216, -0.2000, -0.2706,  ..., -0.0667, -0.0824, -0.0745],\n",
      "          ...,\n",
      "          [ 0.0039, -0.0431, -0.1137,  ..., -0.4196, -0.4275, -0.4118],\n",
      "          [-0.0980, -0.1059, -0.1294,  ..., -0.4510, -0.4510, -0.4275],\n",
      "          [-0.1608, -0.1529, -0.1608,  ..., -0.4745, -0.4588, -0.4275]],\n",
      "\n",
      "         [[-0.4118, -0.4980, -0.5843,  ..., -0.4902, -0.5608, -0.6392],\n",
      "          [-0.4745, -0.5529, -0.6157,  ..., -0.4980, -0.5294, -0.5529],\n",
      "          [-0.5373, -0.6000, -0.6549,  ..., -0.4667, -0.5059, -0.5216],\n",
      "          ...,\n",
      "          [-0.5529, -0.6471, -0.7490,  ..., -0.8431, -0.8353, -0.8353],\n",
      "          [-0.6471, -0.6784, -0.7176,  ..., -0.8510, -0.8353, -0.8196],\n",
      "          [-0.7098, -0.6863, -0.6784,  ..., -0.8745, -0.8588, -0.8275]]],\n",
      "\n",
      "\n",
      "        [[[-0.4039, -0.4824, -0.5765,  ..., -0.0118,  0.1451,  0.2627],\n",
      "          [-0.4510, -0.4745, -0.5216,  ...,  0.0039,  0.1765,  0.3255],\n",
      "          [-0.6000, -0.5529, -0.5137,  ...,  0.0902,  0.2784,  0.4039],\n",
      "          ...,\n",
      "          [-0.6627, -0.7725, -0.8353,  ..., -0.7490, -0.7569, -0.7725],\n",
      "          [-0.5373, -0.7647, -0.8510,  ..., -0.7647, -0.7647, -0.7725],\n",
      "          [-0.3490, -0.7020, -0.8431,  ..., -0.7804, -0.7804, -0.7804]],\n",
      "\n",
      "         [[-0.6549, -0.7098, -0.7647,  ...,  0.2784,  0.3176,  0.3333],\n",
      "          [-0.6627, -0.7020, -0.7490,  ...,  0.2863,  0.3333,  0.3490],\n",
      "          [-0.7412, -0.7412, -0.7412,  ...,  0.3098,  0.3490,  0.3725],\n",
      "          ...,\n",
      "          [-0.7412, -0.8275, -0.8745,  ..., -0.8510, -0.8588, -0.8510],\n",
      "          [-0.6941, -0.8275, -0.8745,  ..., -0.8588, -0.8588, -0.8510],\n",
      "          [-0.6235, -0.7725, -0.8510,  ..., -0.8510, -0.8588, -0.8588]],\n",
      "\n",
      "         [[-0.6627, -0.7412, -0.8039,  ...,  0.6784,  0.6471,  0.5294],\n",
      "          [-0.7098, -0.7725, -0.8118,  ...,  0.6784,  0.6000,  0.4745],\n",
      "          [-0.7882, -0.8118, -0.8196,  ...,  0.6471,  0.5451,  0.4196],\n",
      "          ...,\n",
      "          [-0.8431, -0.8745, -0.8980,  ..., -0.9137, -0.9137, -0.9216],\n",
      "          [-0.8588, -0.8824, -0.8980,  ..., -0.9216, -0.9137, -0.9137],\n",
      "          [-0.8902, -0.8745, -0.8902,  ..., -0.9216, -0.9137, -0.9059]]]])\n",
      "tensor([[   1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   4,    4,    4,   50,    4,    4,   50,    4,   50,   50,   71,    4,\n",
      "            4,    4,    4,   50,    4,    4, 1092,  166,  204,    4,    4,    4,\n",
      "            4,   50,    4,  465,    4,   10,    4,    4],\n",
      "        [ 312,  217,   85,  258,   21,    6,   96,    6,  516,  134,   61,   80,\n",
      "           61,  605,  196,   51,   80,   14,    6,  575,   34,   61, 1586,    6,\n",
      "           56,   51,    3,   17,  668,    9,   20,  180],\n",
      "        [1852,  726,    6,  119,   16, 1726,    8,  156,  209,  598,   68,    8,\n",
      "           80,  117,   80, 1091,  410,    8, 2216, 1921,   50,  145,  564,   88,\n",
      "           12,   34,   17,  103,    8,   43,   16,  726],\n",
      "        [   3,   17,  111,   13,   30,   60,   38,  502,   99,   27,  953,    4,\n",
      "           17,  891,   75,   37,   99,    4,    8,  258,   51,    7,   29,    4,\n",
      "         2887,  927,    3,    4,   10,   17,   21,    3],\n",
      "        [  67,  103,   37,    4,    6, 1044,   34,   19,   10,  499,    8,  195,\n",
      "           70,   90,    8,  230,  890,   20,  192,   34,    8,   69,   13,  207,\n",
      "          421,    8,   67, 1183, 2329,    8,    6,  155],\n",
      "        [  27,   67,    4,  213,   17,    8, 1123,   10,  180,  109,    4,   83,\n",
      "            8,   24,   23,   22,   12,  599,    5,   41,   10,    8,   10,   36,\n",
      "           81,    4,    4,  130,    2,   10,  247,    4],\n",
      "        [   4,   13,   72,  492,   69,    4,   12,   52,    5,    4,  953,   16,\n",
      "           10,  439,   11,   16,    4,   17,    2,    3,   31,   23,  702,   13,\n",
      "           27,   42, 2959,  300,    0,  159,    3,  180],\n",
      "        [   6,  516,   42,   90,   13,  531,   50,    5,    2,  383, 1132,  777,\n",
      "           31,  124,    4,  182,  539, 1095,    0,    8,   16,   11,  419,   10,\n",
      "           10,    5,   11,   17,    0,  128,    4,   45],\n",
      "        [   5,    8,    5,    2,  136,   12,   96,    2,    0, 2060,    2,  127,\n",
      "           79,  243,  446,    5,   11,   13,    0,   10,  120,    4,    4,   52,\n",
      "          617,    2,  625,  392,    0,   12,   30,  106],\n",
      "        [   2, 2903,    2,    0,  308,  606,    8,    0,    0,   13,    0,   81,\n",
      "            4,    4,   11,    2,    4,    4,    0,  590,  247,   53,  177,    5,\n",
      "            5,    0,    5,    5,    0,   24,    6,  103],\n",
      "        [   0,    5,    0,    0,  608,  524,   30,    0,    0,    4,    0,   40,\n",
      "           18,   57,   44,    0,    3,  250,    0,    2,  211,  685,    5,    2,\n",
      "            2,    0,    2,    2,    0,  414,   27,    4],\n",
      "        [   0,    2,    0,    0,   47,    8,   13,    0,    0,   53,    0,    4,\n",
      "            5,  451,   12,    0,    5,  966,    0,    0,    8,    8,    2,    0,\n",
      "            0,    0,    0,    0,    0,   80,   10, 1566],\n",
      "        [   0,    0,    0,    0,   27,  136,    4,    0,    0,  446,    0,    3,\n",
      "            2,    5,   44,    0,    2,    5,    0,    0,   60,  171,    0,    0,\n",
      "            0,    0,    0,    0,    0,    5,  148,   11],\n",
      "        [   0,    0,    0,    0,    4, 1849,  834,    0,    0,   11,    0,   33,\n",
      "            0,    2, 1105,    0,    0,    2,    0,    0,   62,    2,    0,    0,\n",
      "            0,    0,    0,    0,    0,    2,    8,   64],\n",
      "        [   0,    0,    0,    0,   80,    5,    5,    0,    0,   44,    0,  158,\n",
      "            0,    0,    5,    0,    0,    0,    0,    0,    5,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,   23,    5],\n",
      "        [   0,    0,    0,    0,   16,    2,    2,    0,    0,    5,    0,  115,\n",
      "            0,    0,    2,    0,    0,    0,    0,    0,    2,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,   11,    2],\n",
      "        [   0,    0,    0,    0,    4,    0,    0,    0,    0,    2,    0,    4,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    4,    0],\n",
      "        [   0,    0,    0,    0,  269,    0,    0,    0,    0,    0,    0,   53,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    3,    0],\n",
      "        [   0,    0,    0,    0,   12,    0,    0,    0,    0,    0,    0, 1922,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0, 1467,    0],\n",
      "        [   0,    0,    0,    0,    4,    0,    0,    0,    0,    0,    0,  244,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    5,    0],\n",
      "        [   0,    0,    0,    0,    3,    0,    0,    0,    0,    0,    0,    5,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    2,    0],\n",
      "        [   0,    0,    0,    0,  484,    0,    0,    0,    0,    0,    0,    2,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   0,    0,    0,    0,   13,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   0,    0,    0,    0,   64,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   0,    0,    0,    0,    5,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   0,    0,    0,    0,    2,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0]])\n"
     ]
    }
   ],
   "source": [
    "for idx,(imgs,captions) in enumerate(loader):\n",
    "    print(imgs)\n",
    "    print(captions)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_examples(model, device, dataset):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    test_img1 = transform(Image.open(\"test_examples/dog.jpg\").convert(\"RGB\")).unsqueeze(\n",
    "        0\n",
    "    )\n",
    "    print(\"Example 1 CORRECT: Dog on a beach by the ocean\")\n",
    "    print(\n",
    "        \"Example 1 OUTPUT: \"\n",
    "        + \" \".join(model.caption_image(test_img1.to(device), dataset.vocab))\n",
    "    )\n",
    "    test_img2 = transform(\n",
    "        Image.open(\"test_examples/child.jpg\").convert(\"RGB\")\n",
    "    ).unsqueeze(0)\n",
    "    print(\"Example 2 CORRECT: Child holding red frisbee outdoors\")\n",
    "    print(\n",
    "        \"Example 2 OUTPUT: \"\n",
    "        + \" \".join(model.caption_image(test_img2.to(device), dataset.vocab))\n",
    "    )\n",
    "    test_img3 = transform(Image.open(\"test_examples/bus.png\").convert(\"RGB\")).unsqueeze(\n",
    "        0\n",
    "    )\n",
    "    print(\"Example 3 CORRECT: Bus driving by parked cars\")\n",
    "    print(\n",
    "        \"Example 3 OUTPUT: \"\n",
    "        + \" \".join(model.caption_image(test_img3.to(device), dataset.vocab))\n",
    "    )\n",
    "    test_img4 = transform(\n",
    "        Image.open(\"test_examples/boat.png\").convert(\"RGB\")\n",
    "    ).unsqueeze(0)\n",
    "    print(\"Example 4 CORRECT: A small boat in the ocean\")\n",
    "    print(\n",
    "        \"Example 4 OUTPUT: \"\n",
    "        + \" \".join(model.caption_image(test_img4.to(device), dataset.vocab))\n",
    "    )\n",
    "    test_img5 = transform(\n",
    "        Image.open(\"test_examples/horse.png\").convert(\"RGB\")\n",
    "    ).unsqueeze(0)\n",
    "    print(\"Example 5 CORRECT: A cowboy riding a horse in the desert\")\n",
    "    print(\n",
    "        \"Example 5 OUTPUT: \"\n",
    "        + \" \".join(model.caption_image(test_img5.to(device), dataset.vocab))\n",
    "    )\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.backends.cudnn.benchmark = True\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#load_model = False\n",
    "#save_model = False\n",
    "#train_CNN = False\n",
    "## Hyperparameters\n",
    "#embed_size = 256\n",
    "#hidden_size = 256\n",
    "#vocab_size = len(dataset.vocab)\n",
    "#num_layers = 1\n",
    "#learning_rate = 3e-4\n",
    "#num_epochs = 128\n",
    "#step = 0\n",
    "#\n",
    "#\n",
    "#\n",
    "## initialize model, loss etc\n",
    "#model = CNNtoRNN(embed_size, hidden_size, vocab_size, num_layers).to(device)\n",
    "#criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
    "#optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#model.train()\n",
    "#\n",
    "#\n",
    "#for epoch in range(num_epochs):\n",
    "#    #print(epoch)\n",
    "#    #print_examples(model, device, dataset)\n",
    "#    \n",
    "#    for idx, (imgs, captions) in tqdm(\n",
    "#        enumerate(loader), total=len(loader), leave=False\n",
    "#    ):\n",
    "#        imgs = imgs.to(device)\n",
    "#        captions = captions.to(device)\n",
    "#        outputs = model(imgs, captions[:-1])\n",
    "#        loss = criterion(\n",
    "#            outputs.reshape(-1, outputs.shape[2]), captions.reshape(-1)\n",
    "#        )\n",
    "#        step += 1\n",
    "#        #uncomment below for l2 norm, l1 norm put p.pow(1)\n",
    "#        #l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n",
    "#        #loss += 0.00001 * l2_norm\n",
    "#        optimizer.zero_grad()\n",
    "#        loss.backward(loss)\n",
    "#        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print_examples(model,device,dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), 'model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Comparison\n",
    "def load_model(model_name,image_size,augmentation=False):\n",
    "    transform_list=[transforms.Resize((image_size,image_size)),transforms.ToTensor()]\n",
    "    if augmentation:\n",
    "        transform_list+=[transforms.RandomHorizontalFlip(),transforms.RandomRotation(10)]\n",
    "    \n",
    "    transform=transforms.Compose(transform_list)\n",
    "    root=\"Flickr8k/Images/\"\n",
    "    captions=\"Flickr8k/captions.txt\"\n",
    "    dataset=FlickrDataset(root,captions,transform)\n",
    "    dataloader=DataLoader(dataset,batch_size=32,shuffle=False,collate_fn=MyCollate(dataset.vocab.stoi[\"<PAD>\"]))\n",
    "    embed_size = 256\n",
    "    hidden_size = 256\n",
    "    vocab_size = len(dataset.vocab)\n",
    "    num_layers = 1\n",
    "    model=CNNtoRNN(embed_size,hidden_size,vocab_size,num_layers)\n",
    "    model.load_state_dict(torch.load(f'{model_name}.pth',weights_only=True))\n",
    "    model.eval()\n",
    "    return model,dataloader\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu_score(predictions, references):\n",
    "    bleu_scores = []\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        # pred and ref are already lists of tokens\n",
    "        score = sentence_bleu([ref], pred)  # BLEU expects a list of references and a hypothesis\n",
    "        bleu_scores.append(score)\n",
    "    return np.mean(bleu_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model,dataloader,loss_fn,device):\n",
    "    model.to(device)\n",
    "    total_loss=0.0\n",
    "    predictions,references=[],[]\n",
    "    with torch.no_grad():\n",
    "        for images,captions in tqdm(dataloader, desc=\"Evaluating\", unit=\"batch\"):\n",
    "            images=images.to(device)\n",
    "            captions=captions.to(device)\n",
    "            \n",
    "            outputs=model(images,captions[:-1])\n",
    "            loss=loss_fn(outputs.reshape(-1, outputs.shape[2]), captions.reshape(-1))\n",
    "            total_loss+=loss.item()\n",
    "            \n",
    "            vocabulary=dataset.vocab\n",
    "            \n",
    "            # Get the predicted word indices\n",
    "            predicted_tokens = outputs.argmax(dim=-1)  # Get predicted word indices\n",
    "            for caption in predicted_tokens:\n",
    "                words = [vocabulary.itos[idx.item()] for idx in caption if idx.item() != vocabulary.stoi['<PAD>']]\n",
    "                predictions.append(words)\n",
    "\n",
    "            references.extend([[vocabulary.itos[idx.item()] for idx in caption] for caption in captions[:, 1:].cpu()])\n",
    "\n",
    "    avg_loss=total_loss/len(dataloader)\n",
    "    bleu_score=calculate_bleu_score(predictions,references)\n",
    "    \n",
    "    return avg_loss,bleu_score\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(metrics_df):\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    ax1.bar(metrics_df['Model'], metrics_df['Loss'], color='red', alpha=0.6, label='Loss')\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(metrics_df['Model'], metrics_df['BLEU Score'], color='blue', marker='o', label='BLEU Score')\n",
    "\n",
    "    ax1.set_xlabel('Models')\n",
    "    ax1.set_ylabel('Loss', color='red')\n",
    "    ax2.set_ylabel('BLEU Score', color='blue')\n",
    "    plt.title('Model Comparison: Loss vs BLEU Score')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(models_info):\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    loss_fn=nn.CrossEntropyLoss()\n",
    "    metrics=defaultdict(list)\n",
    "    for model_name, config in models_info.items():\n",
    "        print(f\"Evaluating {model_name}...\")\n",
    "        model, dataloader = load_model(model_name, config['image_size'], config['augmentation'])\n",
    "        loss, bleu = evaluate_model(model, dataloader, loss_fn, device)\n",
    "\n",
    "        metrics['Model'].append(model_name)\n",
    "        metrics['Loss'].append(loss)\n",
    "        metrics['BLEU Score'].append(bleu)\n",
    "\n",
    "        print(f\"Model: {model_name}, Loss: {loss:.4f}, BLEU Score: {bleu:.4f}\")\n",
    "\n",
    "    # Convert metrics to DataFrame for better visualization\n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    print(metrics_df)\n",
    "\n",
    "    # Plot comparison\n",
    "    plot_metrics(metrics_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_info={\n",
    "    'model_adam_224':{'image_size':224,'augmentation':False},\n",
    "    'model_adam_with_augmentation':{'image_size':224,'augmentation':True},\n",
    "    'model_adam':{'image_size':256,'augmentation':False},\n",
    "    'model_l1':{'image_size':224,'augmentation':False},\n",
    "    'model_l2':{'image_size':224,'augmentation':False},\n",
    "    'model_sgd_64_iter':{'image_size':224,'augmentation':False}\n",
    "}\n",
    "#compare_models(models_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/esun/Image_Captioner/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/esun/Image_Captioner/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/tmp/ipykernel_5340/4113001996.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('model_adam_with_augmentation.pth'))\n",
      "/home/esun/Image_Captioner/.venv/lib/python3.12/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1 CORRECT: Dog on a beach by the ocean\n",
      "Example 1 OUTPUT: <SOS> a dog walking on a beach . <EOS>\n",
      "Example 2 CORRECT: Child holding red frisbee outdoors\n",
      "Example 2 OUTPUT: <SOS> a little girl in a yellow dress and blue pants is in front of a blue and yellow merry - go - round . <EOS>\n",
      "Example 3 CORRECT: Bus driving by parked cars\n",
      "Example 3 OUTPUT: <SOS> a man is walking next to a building with a red car shaped like it is looking away . <EOS>\n",
      "Example 4 CORRECT: A small boat in the ocean\n",
      "Example 4 OUTPUT: <SOS> a boat floats in the water . <EOS>\n",
      "Example 5 CORRECT: A cowboy riding a horse in the desert\n",
      "Example 5 OUTPUT: <SOS> a man and a woman are sitting on a hill overlooking a lake . <EOS>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "root=\"Flickr8k/Images/\"\n",
    "captions=\"Flickr8k/captions.txt\"\n",
    "dataset=FlickrDataset(root,captions,transform)\n",
    "dataloader=DataLoader(dataset,batch_size=32,shuffle=False,collate_fn=MyCollate(dataset.vocab.stoi[\"<PAD>\"]))\n",
    "embed_size = 256\n",
    "hidden_size = 256\n",
    "vocab_size = len(dataset.vocab)\n",
    "num_layers = 1\n",
    "model=CNNtoRNN(embed_size,hidden_size,vocab_size,num_layers)\n",
    "model.load_state_dict(torch.load('model_adam_with_augmentation.pth'))\n",
    "device='cuda'\n",
    "model.to(device)\n",
    "print_examples(model,device,dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
